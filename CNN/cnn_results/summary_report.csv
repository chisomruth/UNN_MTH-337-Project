Optimizer,Learning_Rate,Final_Train_Loss,Final_Val_Loss,Final_Train_Acc,Final_Val_Acc,Best_Val_Acc,Best_Epoch
ADAM,0.0001,2.002673387527466,2.102499008178711,0.5608333349227905,0.5416666865348816,0.5691666603088379,46
ADAM,0.001,1.104758858680725,1.8356783390045166,0.4612500071525574,0.3591666519641876,0.4808333218097687,38
ADAM,0.01,1.232387900352478,1.1975173950195312,0.38374999165534973,0.3333333432674408,0.4658333361148834,16
ADAM,0.1,1.1971746683120728,1.1848807334899902,0.34583333134651184,0.3375000059604645,0.4399999976158142,38
ADAGRAD,0.0001,3.3213717937469482,3.2552359104156494,0.40833333134651184,0.47833332419395447,0.5024999976158142,38
ADAGRAD,0.001,3.142353057861328,3.2249412536621094,0.5295833349227905,0.5266666412353516,0.528333306312561,42
ADAGRAD,0.01,2.686537504196167,2.7425570487976074,0.5679166913032532,0.5533333420753479,0.5533333420753479,50
ADAGRAD,0.1,1.2282809019088745,1.6359364986419678,0.5287500023841858,0.44333332777023315,0.5375000238418579,45
RMSPROP,0.0001,1.866255283355713,2.0147976875305176,0.5891666412353516,0.4791666567325592,0.5833333134651184,46
RMSPROP,0.001,1.0819673538208008,1.2763288021087646,0.4633333384990692,0.3400000035762787,0.5058333277702332,42
RMSPROP,0.01,1.134679913520813,1.1096224784851074,0.3479166626930237,0.3333333432674408,0.4333333373069763,26
RMSPROP,0.1,2388.230224609375,647.0806274414062,0.3408333361148834,0.3375000059604645,0.39250001311302185,7
SGD,0.0001,3.231806755065918,3.242100715637207,0.4791666567325592,0.5049999952316284,0.5133333206176758,46
SGD,0.001,3.000666856765747,3.3624556064605713,0.5612499713897705,0.4241666793823242,0.5325000286102295,35
SGD,0.01,1.9893672466278076,2.128748893737793,0.5733333230018616,0.5074999928474426,0.5408333539962769,42
SGD,0.1,1.076438307762146,1.529721736907959,0.4749999940395355,0.3683333396911621,0.5291666388511658,48
