Optimizer,Learning_Rate,Final_Train_Loss,Final_Val_Loss,Best_Val_Loss,Best_Epoch,Test_MSE,Test_RMSE,Test_MAE,Test_R2
ADAM,0.0001,0.57,0.13,0.12,78,"454,751,120.36","21,324.89","15,590.28",0.8103
ADAM,0.001,0.18,0.08,0.08,99,"268,989,006.40","16,400.88","10,570.65",0.8878
ADAM,0.01,0.12,0.08,0.07,46,"239,101,100.36","15,462.89","9,962.99",0.9003
ADAM,0.1,0.20,0.11,0.08,45,"264,900,966.64","16,275.78","10,746.25",0.8895
ADAGRAD,0.0001,2.18,0.83,0.83,100,"2,555,379,002.25","50,550.76","40,811.67",-0.0658
ADAGRAD,0.001,0.65,0.13,0.13,77,"512,238,739.41","22,632.69","16,084.12",0.7864
ADAGRAD,0.01,0.22,0.09,0.08,89,"271,876,551.98","16,488.68","11,393.75",0.8866
ADAGRAD,0.1,0.14,0.09,0.07,83,"246,665,396.99","15,705.58","10,580.66",0.8971
RMSPROP,0.0001,0.45,0.11,0.11,97,"419,132,041.49","20,472.71","14,551.89",0.8252
RMSPROP,0.001,0.16,0.08,0.07,57,"243,924,704.16","15,618.09","9,605.54",0.8983
RMSPROP,0.01,0.13,0.17,0.08,66,"371,703,584.83","19,279.62","13,383.66",0.8450
RMSPROP,0.1,0.58,5.10,0.10,83,"9,654,039,693.14","98,254.97","64,096.90",-3.0266
SGD,0.0001,0.41,0.08,0.08,100,"409,419,821.58","20,234.13","12,806.40",0.8292
SGD,0.001,0.20,0.08,0.07,61,"304,285,029.89","17,443.77","10,723.82",0.8731
SGD,0.01,0.16,0.08,0.07,50,"215,327,216.93","14,674.03","9,370.24",0.9102
SGD,0.1,0.21,0.38,0.08,79,"747,653,768.20","27,343.26","22,076.47",0.6882
